{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Ensemble Methods?\n",
    "\n",
    "It is a machine learning technique that involves combining several basic models in order to produce a better model. Here by better, we mean a model that predicts the output variable of the test dataset with more accuracy. To better explain the concept, we will look at an example.\n",
    "\n",
    "**The problem statement and data for this example has been taken from my 10-601 Introduction to Machine Learning course for Spring 2018 assignment.**\n",
    "\n",
    "## Problem Statement:\n",
    "\n",
    "To predict the final grade (A, not A) for high school students based on the following features/attributes: <br>\n",
    "The student grades on 5 multiple choice assignments M1 through M5, 4 programming assignments P1 through P4, and the final exam F.\n",
    "\n",
    "I will import the excel file using pandas to show how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>F</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>notA</td>\n",
       "      <td>notA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     M1    M2    M3    M4 M5    P1    P2    P3    P4     F grade\n",
       "0  notA  notA     A  notA  A     A     A  notA  notA     A     A\n",
       "1  notA     A     A  notA  A  notA  notA  notA     A  notA  notA\n",
       "2  notA     A     A     A  A  notA     A  notA  notA     A     A\n",
       "3  notA  notA  notA  notA  A     A     A  notA  notA     A  notA\n",
       "4     A  notA     A     A  A     A     A  notA  notA     A     A"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Path = 'C:\\\\Users\\\\deeprob\\\\10-601\\\\HW2\\\\'\n",
    "train_data = pd.read_csv(Path+'education_train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataset, M1,M2,M3,M4,M5 are the 5 multiple choice assignment and the grades received by the student is indicated in their respective columns. Similarly, P1,P2,P3,P4 are the programming assignments, F is the final exam and grade is the final grade of the student that we want to predict.\n",
    "\n",
    "## Data Preprocessing:\n",
    "\n",
    "First, we need to convert the columns to numerical values in order to make our algorithm work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.replace('notA',0,inplace=True)\n",
    "train_data.replace('A',1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will solve this problem using two methods.**\n",
    "\n",
    "## Problem Solution 1:\n",
    "In the first method, we will fit a simple Decision Tree classifier to the training data. The specifications of the Decision Tree are as follows:<br>\n",
    "1. **criterion**: entropy or information gain\n",
    "2. **max_depth**: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train_data.iloc[:,:-1]\n",
    "ytrain = train_data.iloc[:,-1]\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = 3)\n",
    "clf = clf.fit(Xtrain.values,ytrain.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we will test the accuracy of this model on the test data.\n",
    "To do that, we will import and process the test data similar to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the simple Decision Tree classifier is: 0.795\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(Path + 'education_test.csv')\n",
    "test_data.replace('notA',0,inplace=True)\n",
    "test_data.replace('A',1,inplace=True)\n",
    "\n",
    "Xtest = test_data.iloc[:,:-1]\n",
    "ytest = test_data.iloc[:,-1]\n",
    "\n",
    "score_simple = clf.score(Xtest,ytest)\n",
    "print(f'The accuracy for the simple Decision Tree classifier is: \\\n",
    "{score_simple}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Solution 2:\n",
    "In the second method, we will try to improve the model by using an ensemble method called the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the random forest classifier is: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=10,criterion='entropy',\n",
    "                              max_depth=3,bootstrap=True,\n",
    "                              random_state=8)\n",
    "clf2.fit(Xtrain.values,ytrain.values)\n",
    "print(f'The accuracy of the random forest classifier is: \\\n",
    "{clf2.score(Xtest,ytest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What exactly happened here?\n",
    "As we can see, the accuracy on the test dataset increased from 0.795 to 0.9. Therefore, we can say that this model is 'better' than the previous model. \n",
    "\n",
    "### But how did this happen and what exactly is a Random Forest? \n",
    "Like a forest that consists of many trees, a Random Forest is nothing but a collection of Decision Trees. Each individual tree predicts an outcome for each test case like in solution 1 and the model's prediction is the outcome with the most votes. For eg. for the 1st test case, the simple Decision Tree predicts 'A'. Similarly, we have N-1 other Decision Trees that also give some predictions for that test case. A Random Forest takes into account the predictions of all N Decision Trees and finally predicts the outcome that is most likely out of those N outcomes. How it works is exactly like a jury. Maybe one individual in the jury gives a wrong verdict but the jury as a whole has more chance of giving the correct verdict because they can overrule that one person who gave the wrong verdict.  \n",
    "\n",
    "### How do we create N-Decision Trees?\n",
    "1. We take our training data and we divide it randomly into N samples. \n",
    "2. For each sample we create a Decision Tree.\n",
    "Thus, we will have N Decision Trees. \n",
    "\n",
    "### How can N-Decision Trees be better than 1 Decision Tree?\n",
    "A single decision tree may suffer from overfitting. But, N Decision Trees that consider the average of all the predictions tend to cancel each others error by *reducing the variance* as long as they all don't err in the same direction. For that, the trees need to be highly uncorrelated.\n",
    "\n",
    "*Variance error is variability of a target function's form with respect to different training sets. Models with small variance error will not change much if you replace couple of samples in training set. Models with high variance might be affected even with small changes in training set.*\n",
    "\n",
    "### How to make the Decision Trees highly uncorrelated?\n",
    "To do that, we use a technique called **bagging** that selects a random number of samples with replacement from the original dataset and creates a Decision Tree based on these samples. Another method that is used is called **feature bagging or feature randomness** where instead of considering every possible feature while splitting a node, we only take into account a random subset of feature for each tree. This can reduce correlation because if we take into account all the features and there is one feature that is of really high importance, then all the trees will split on that feature and that will result in a bunch of trees that behave similarly. We can avoid it using feature bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Solution 3:\n",
    "In the third method, we go a step further and use feature bagging as well to further increase the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the random forest classifier is: 0.915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf3 = ExtraTreesClassifier(n_estimators=10,criterion='entropy',\n",
    "                            max_depth=3,bootstrap=True,random_state=47)\n",
    "clf3.fit(Xtrain.values,ytrain.values)\n",
    "\n",
    "print(f'The accuracy of the random forest classifier is: \\\n",
    "{clf3.score(Xtest,ytest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and Sequential Ensemble Methods\n",
    "\n",
    "What we have seen till now is just one family of ensemble methods called ***parallel ensemble methods***. Here, the basic models are generated in parallel and we take advantage of their independence or uncorrelation to come up with a better model. This strategy is also called bagging. Eg. Random Forest. There is another family of methods called the ***sequential ensemble methods.***\n",
    "\n",
    "## Sequential Ensemble Methods:\n",
    "Unlike the parallel ensemble method that focussed on bagging, the sequential ensemble methods focuses on a technique called boosting. Adaboost is the most widely used form of boosting algorithm and we are going to cover it in details.\n",
    "\n",
    "### What is AdaBoost?\n",
    "It is a boosting algorithm that fits a sequence of weak algorithms on repeatedly modified versions of the data. \n",
    "\n",
    "### How does it work?\n",
    "1. Initially we assign weights to all the samples and set those weight to 1/N.\n",
    "2. For each iteration, we modify the weights. Those examples that were incorrectly classified are given more weightage while those that are correctly classified have their weights decreased. Therefore we are increasing the influence of the examples that are difficult to predict at each successive iteration.\n",
    "3. We repeat the learning algorithm to the reweighted data.\n",
    "\n",
    "### Why is it better than a simple DecisionTree?\n",
    "As opposed to bagging that reduces the variance, boosting *reduces the bias*.\n",
    "\n",
    "*Bias error is due to our assumptions about target function. The more assumptions(restrictions) we make about target functions, the more bias we introduce. Models with high bias are less flexible because we have imposed more rules on the target functions.*\n",
    "\n",
    "### Solution 4:\n",
    "Here, we will use Adaboost to increase the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the AdaBoost classifier is: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf4 = AdaBoostClassifier(clf,n_estimators=5,random_state=0)\n",
    "clf4.fit(Xtrain.values,ytrain.values)\n",
    "\n",
    "print(f'The accuracy of the AdaBoost classifier is: \\\n",
    "{clf4.score(Xtest,ytest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
    "2. https://blog.statsbot.co/ensemble-learning-d1dcd548e936#targetText=Ensemble%20methods%20are%20meta%2Dalgorithms,or%20improve%20predictions%20(stacking).\n",
    "3. https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249\n",
    "4. https://www.datacamp.com/community/tutorials/random-forests-classifier-python#comparison\n",
    "5. https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n",
    "6. https://stats.stackexchange.com/questions/262794/why-does-a-decision-tree-have-low-bias-high-variance\n",
    "7. https://scikit-learn.org/stable/modules/ensemble.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
